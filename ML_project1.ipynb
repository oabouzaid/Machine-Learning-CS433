{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "# constants\n",
    "training_data = '/Users/karunya/Documents/EPFL/ML/Project_1/train.csv'\n",
    "test_data = '/Users/karunya/Documents/EPFL/ML/Project_1/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(y, tx, w):\n",
    "    \"\"\"Calculates the error in current prediction.\"\"\"\n",
    "    return y - np.dot(tx, w)\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculates the loss using MSE.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = 1/(2*N)\n",
    "    loss = (np.dot(np.transpose(e), e)) * factor\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient of the MSE loss function.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = -1/N\n",
    "    grad = (np.dot(np.transpose(tx), e)) * factor\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Computes a stochastic gradient from a few examples n and their corresponding y_n labels.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = -1/N\n",
    "    grad = (np.dot(np.transpose(tx), e)) * factor\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "def sigma(x):\n",
    "    \"\"\"Calculates sigma using the formula.\"\"\"\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables - the output desired values 'y' and the input data 'tx'.\n",
    "    Outputs an iterator which gives mini-batches of batch_size matching elements from y and tx.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use:\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        do something\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "            \n",
    "def compare_prediction(w_train, x, y):\n",
    "    \"\"\"Calculates accuracy by comparing prediction with given test data.\"\"\"\n",
    "    pred = predict_labels(w_train, x)\n",
    "    N = len(pred)\n",
    "    count = 0.0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == y[i]:\n",
    "            count += 1\n",
    "    # matches = (y == pred).sum()\n",
    "    return count/N\n",
    "\n",
    "\n",
    "def split_data(tx, ty, ratio, seed=1):\n",
    "    \"\"\"Split training data by ratio\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    split_idxs = [i for i in range(len(tx))]\n",
    "    \n",
    "    # Shuffle the indicies randomly\n",
    "    np.random.shuffle(split_idxs)\n",
    "    tx_shuffled = tx[split_idxs]\n",
    "    ty_shuffled = ty[split_idxs]\n",
    "    \n",
    "    # Split by ratio\n",
    "    split_pos = int(len(tx) * ratio)\n",
    "    x_train = tx_shuffled[:split_pos]\n",
    "    x_test = tx_shuffled[split_pos:]\n",
    "    y_train = ty_shuffled[:split_pos]\n",
    "    y_test = ty_shuffled[split_pos:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"Builds a polynomial of the given degree and appends it to the given matrix.\"\"\"\n",
    "    x_ret = x\n",
    "    for i in range(2, degree+1):\n",
    "        x_ret = np.c_[x_ret, np.power(x, i)]\n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardizes the matrix by calculating the mean of each column and subtracting it from individual values.\"\"\"\n",
    "    x_ret = x.copy()\n",
    "    for i in range(0, x.shape[1]):\n",
    "        if i==22:\n",
    "            continue\n",
    "        x_ret[:,i] -= np.mean(x_ret[:,i])\n",
    "        x_ret[:,i] /= np.std(x_ret[:,i])\n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def replace_999(x):\n",
    "    x[x == -999.0] = np.nan\n",
    "    col_mean = np.nanmean(x, axis=0)\n",
    "    inds = np.where(np.isnan(x))\n",
    "    x[inds] = np.take(col_mean, inds[1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def one_hot_encoding(x):\n",
    "    col = x[:,22]\n",
    "    b = np.zeros((x.shape[0], 4))\n",
    "    for i in range(len(b)):\n",
    "        a = int(col[i])\n",
    "        b[i][a] = 1\n",
    "    x = np.delete(x, 22, 1)\n",
    "    x = np.hstack((x, b))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to Implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculates the solution using the least squares method.\"\"\"     \n",
    "    gram = np.dot(np.transpose(tx),tx)\n",
    "    gram = np.linalg.inv(gram)\n",
    "    \n",
    "    w = np.dot(gram,np.transpose(tx))\n",
    "    w = np.dot(w, y)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the least squares with gradient descent method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad, loss = compute_gradient(y, tx ,w)\n",
    "        w = w - gamma * grad  \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the least squares with stochastic gradient descent method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            w = w - gamma * grad\n",
    "            loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Calculates the solution using the ridge regression method.\"\"\"\n",
    "    N = tx.shape[1]\n",
    "    a = np.dot(np.transpose(tx), tx) + lambda_ * np.identity(N)\n",
    "    b = np.dot(np.transpose(tx), y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the logistic regression method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        yx = np.dot(y, tx)\n",
    "        yxw = np.dot(yx, w)\n",
    "        log = np.log(1 + np.exp(np.dot(tx, w)))\n",
    "        loss = (log - yxw).sum()\n",
    "        \n",
    "        # Update rule\n",
    "        sig = sigma(np.dot(tx, w))\n",
    "        sig = sig - y\n",
    "        grad = np.dot(np.transpose(tx), sig)\n",
    "        w = w - gamma * grad \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_ , initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the regularized logistic regression using gradient descent method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        yx = np.dot(y, tx)\n",
    "        yxw = np.dot(yx, w)\n",
    "        log = np.log(1 + np.exp(np.dot(tx, w)))\n",
    "        \n",
    "        # Add the 'penalty' term\n",
    "        loss = (log - yxw).sum() - (lambda_/2)* np.square((np.linalg.norm(w)))\n",
    "        \n",
    "        # Update rule\n",
    "        sig = sigma(np.dot(tx, w))\n",
    "        sig = sig - y\n",
    "        grad = np.dot(np.transpose(tx), sig) + 2 * lambda_*w\n",
    "        w = w - gamma * grad\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Retrieve input training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty, tx, ids_train = load_csv_data(training_data, sub_sample = False)\n",
    "\n",
    "fy, fx, ids_test = load_csv_data(test_data, sub_sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_tx = tx.copy()\n",
    "orig_ty = ty.copy()\n",
    "orig_fx = fx.copy()\n",
    "\n",
    "# replace 999's\n",
    "x_train = replace_999(tx)\n",
    "fx_train = replace_999(fx)\n",
    "\n",
    "# normalize data\n",
    "x_train = standardize(x_train)\n",
    "fx_train = standardize(fx_train)\n",
    "\n",
    "# split data by 80-20\n",
    "x_train, y_train, x_test, y_test = split_data(x_train, ty, 0.80, seed=1)\n",
    "\n",
    "# hot-encoding\n",
    "x_train = one_hot_encoding(x_train)\n",
    "x_test = one_hot_encoding(x_test)\n",
    "fx_train = one_hot_encoding(fx_train)\n",
    "\n",
    "# add intercept\n",
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "fx_train = np.hstack((np.ones((fx_train.shape[0], 1)), fx_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Least Squares Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training 'test set': 66.482%\n",
      "Accuracy with actual test set: 65.11496943182257%\n"
     ]
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "ls_accuracy = compare_prediction(w_ls, x_test, y_test)\n",
    "print('Accuracy with training \\'test set\\': ' + str(ls_accuracy * 100) + '%')\n",
    "\n",
    "ls_accuracy = compare_prediction(w_ls, fx_train, fy)\n",
    "print('Accuracy with actual test set: ' + str(ls_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares using Gradient Descent (using MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training 'test set': 53.080000000000005%\n",
      "Accuracy with actual test set: 56.79380822824239%\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10\n",
    "step_size = 1e-1\n",
    "w_0 = np.ones(x_train.shape[1])\n",
    "\n",
    "w_lsgd, loss_lsgd = least_squares_GD(y_train, x_train, w_0, max_iters, step_size)\n",
    "lsGD_accuracy = compare_prediction(w_lsgd, x_test, y_test)\n",
    "print('Accuracy with training \\'test set\\': ' + str(lsGD_accuracy * 100) + '%')\n",
    "\n",
    "lsGD_accuracy = compare_prediction(w_lsgd, fx_train, fy)\n",
    "print('Accuracy with actual test set: ' + str(lsGD_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares using Stochastic Gradient Descent (batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training 'test set': 33.995999999999995%\n",
      "Accuracy with actual test set: 65.88471731915149%\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10\n",
    "step_size = 1e-1\n",
    "w_0 = np.zeros(x_train.shape[1])\n",
    "\n",
    "w_lstoch, loss_lstoch = least_squares_SGD(y_train, x_train, w_0, max_iters, step_size) \n",
    "lsStoch_accuracy = compare_prediction(w_lstoch, x_test, y_test)\n",
    "print('Accuracy with training \\'test set\\': '  + str(lsStoch_accuracy * 100) + '%')\n",
    "\n",
    "lsStoch_accuracy = compare_prediction(w_lstoch, fx_train, fy)\n",
    "print('Accuracy with actual test set: ' + str(lsStoch_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 12\n",
      "Lambda: 0.15256\n",
      "Accuracy with training 'test set': 81.7808%\n",
      "Accuracy with test set: 30.571521087994817%\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "degree = 12\n",
    "lambda_ = 0.15256\n",
    "\n",
    "# replace 999's\n",
    "x_train = replace_999(tx)\n",
    "fx_train = replace_999(fx)\n",
    "# print('Replacing 999: ' + str(x_train.shape))\n",
    "\n",
    "# normalize data\n",
    "x_train = standardize(x_train)\n",
    "fx_train = standardize(fx_train)\n",
    "# print('Normalize: ' + str(x_train.shape))\n",
    "\n",
    "# split data by 80-20\n",
    "x_train, y_train, x_test, y_test = split_data(x_train, ty, 0.75, seed=1)\n",
    "\n",
    "# hot-encoding\n",
    "x_train = one_hot_encoding(x_train)\n",
    "x_test = one_hot_encoding(x_test)\n",
    "fx_train = one_hot_encoding(fx_train)\n",
    "# print('One-hot Encoding: ' + str(x_train.shape))\n",
    "\n",
    "# polynomial-fit\n",
    "x_train = build_poly(x_train, degree)\n",
    "x_test = build_poly(x_test, degree)\n",
    "fx_train = build_poly(fx_train, degree)\n",
    "# print('Build Poly: ' + str(x_train.shape))\n",
    "\n",
    "# add intercept\n",
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "fx_train = np.hstack((np.ones((fx_train.shape[0], 1)), fx_train))\n",
    "# print('Add intercept: ' + str(x_train.shape))\n",
    "\n",
    "w_rr, loss_rr = ridge_regression(y_train, x_train, lambda_)\n",
    "rr_accuracy = compare_prediction(w_rr, x_test, y_test)\n",
    "\n",
    "print('Degree: ' + str(degree))\n",
    "print('Lambda: ' + str(lambda_))\n",
    "print('Accuracy with training \\'test set\\': '  + str(rr_accuracy * 100) + '%')\n",
    "\n",
    "rr_accuracy = compare_prediction(w_rr, fx_train, fy)\n",
    "print('Accuracy with test set: '  + str(rr_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training 'test set': 65.5808%\n",
      "Accuracy with test set: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "max_iters = 100\n",
    "step_size = 3\n",
    "\n",
    "# replace 999's\n",
    "x_train = replace_999(orig_tx)\n",
    "fx_train = replace_999(orig_fx)\n",
    "\n",
    "# normalize data\n",
    "# x_train = standardize(x_train)\n",
    "# fx_train = standardize(fx_train)\n",
    "\n",
    "# split data by 80-20\n",
    "x_train, y_train, x_test, y_test = split_data(x_train, ty, 0.75, seed=1)\n",
    "\n",
    "# hot-encoding\n",
    "# x_train = one_hot_encoding(x_train)\n",
    "# x_test = one_hot_encoding(x_test)\n",
    "# fx_train = one_hot_encoding(fx_train)\n",
    "\n",
    "# add intercept\n",
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "fx_train = np.hstack((np.ones((fx_train.shape[0], 1)), fx_train))\n",
    "\n",
    "w_0 = np.zeros(x_train.shape[1])\n",
    "\n",
    "w_lrr, loss_rr = logistic_regression(y_train, x_train, w_0, max_iters, step_size)\n",
    "\n",
    "lrr_accuracy = compare_prediction(w_lrr, x_test, y_test)\n",
    "print('Accuracy with training \\'test set\\': '  + str(lrr_accuracy * 100) + '%')\n",
    "\n",
    "lrr_accuracy = compare_prediction(w_lrr, fx_train, fy)\n",
    "print('Accuracy with test set: '  + str(lrr_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training 'test set': 0.0%\n",
      "Accuracy with test set: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "lambda_ = 0.15256\n",
    "max_iters = 10\n",
    "step_size = 0.003\n",
    "\n",
    "# replace 999's\n",
    "x_train = replace_999(orig_tx)\n",
    "fx_train = replace_999(orig_fx)\n",
    "\n",
    "# # normalize data\n",
    "# x_train = standardize(x_train)\n",
    "# fx_train = standardize(fx_train)\n",
    "\n",
    "# split data by 80-20\n",
    "x_train, y_train, x_test, y_test = split_data(x_train, ty, 0.75, seed=1)\n",
    "\n",
    "# hot-encoding\n",
    "x_train = one_hot_encoding(x_train)\n",
    "x_test = one_hot_encoding(x_test)\n",
    "fx_train = one_hot_encoding(fx_train)\n",
    "\n",
    "# add intercept\n",
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "fx_train = np.hstack((np.ones((fx_train.shape[0], 1)), fx_train))\n",
    "\n",
    "w_0 = np.zeros(x_train.shape[1])\n",
    "\n",
    "w_rlr, loss_rr = reg_logistic_regression(y_train, x_train, lambda_ , w_0, max_iters, step_size)\n",
    "\n",
    "rlr_accuracy = compare_prediction(w_rlr, x_test, y_test)\n",
    "print('Accuracy with training \\'test set\\': '  + str(lrr_accuracy * 100) + '%')\n",
    "\n",
    "rlr_accuracy = compare_prediction(w_rlr, fx_train, fy)\n",
    "print('Accuracy with test set: '  + str(rlr_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions to Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = predict_labels(w_rr, fx_train)\n",
    "# print('Predictions shape: ' + str(y_pred.shape))\n",
    "# create_csv_submission(ids_test, y_pred, \"output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
