{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "# constants\n",
    "training_data = '/Users/Gaurav/Desktop/train.csv'\n",
    "test_data = '/Users/Gaurav/Desktop/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(y, tx, w):\n",
    "    \"\"\"Calculates the error in current prediction.\"\"\"\n",
    "    return y - np.dot(tx, w)\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculates the loss using MSE.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = 1/(2*N)\n",
    "    loss = (np.dot(np.transpose(e), e)) * factor\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient of the MSE loss function.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = -1/N\n",
    "    grad = (np.dot(np.transpose(tx), e)) * factor\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Computes a stochastic gradient from a few examples n and their corresponding y_n labels.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = -1/N\n",
    "    grad = (np.dot(np.transpose(tx), e)) * factor\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables - the output desired values 'y' and the input data 'tx'.\n",
    "    Outputs an iterator which gives mini-batches of batch_size matching elements from y and tx.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use:\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        do something\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "            \n",
    "def compare_prediction(w_train, x, y):\n",
    "    \"\"\"Calculates accuracy by comparing prediction with given test data.\"\"\"\n",
    "    pred = predict_labels(w_train, x)\n",
    "    N = len(pred)\n",
    "    count = 0.0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == y[i]:\n",
    "            count += 1\n",
    "    # matches = (y == pred).sum()\n",
    "    return count/N\n",
    "\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # split the data based on the given ratio\n",
    "    idx = [i for i in range(len(x))]\n",
    "    np.random.shuffle(idx)\n",
    "    split = int(len(x) * ratio)\n",
    "    \n",
    "    x_shuffle = x[idx]\n",
    "    y_shuffle = y[idx]\n",
    "    \n",
    "    x_train = x_shuffle[:split]\n",
    "    x_test = x_shuffle[split:]\n",
    "    y_train = y_shuffle[:split]\n",
    "    y_test = y_shuffle[split:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"Builds a polynomial of the given degree and appends it to the given matrix.\"\"\"\n",
    "    x_ret = x\n",
    "    for i in range(2, degree+1):\n",
    "        x_ret = np.c_[x_ret, np.power(x, i)]\n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardizes the matrix by calculating the mean of each column and subtracting it from individual values.\"\"\"\n",
    "    x_ret = x.copy()\n",
    "    for i in range(1, x.shape[1]):\n",
    "        c = x[:,i]\n",
    "        c = (c - np.mean(c))/np.std(c)\n",
    "        x_ret[:,i] = c\n",
    "    return x_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to Implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculates the solution using the least squares method.\"\"\"     \n",
    "    gram = np.dot(np.transpose(tx),tx)\n",
    "    gram = np.linalg.inv(gram)\n",
    "    \n",
    "    w = np.dot(gram,np.transpose(tx))\n",
    "    w = np.dot(w, y)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the least squares with gradient descent method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad, loss = compute_gradient(y, tx ,w)\n",
    "        w = w - gamma * grad  \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the least squares with stochastic gradient descent method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            w = w - gamma * grad\n",
    "            loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Calculates the solution using the ridge regression method.\"\"\"\n",
    "    N = tx.shape[1]\n",
    "    a = np.dot(np.transpose(tx), tx) + lambda_ * np.identity(N)\n",
    "    b = np.dot(np.transpose(tx), y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def sigma(x):\n",
    "    \"\"\"Calculates sigma using the formula.\"\"\"\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the logistic regression method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        yx = np.dot(y, np.transpose(tx))\n",
    "        yxw = np.dot(yx, w)\n",
    "        log = np.log(1 + np.exp(np.dot(np.transpose(tx), w)))\n",
    "        loss = (log - yxw).sum()\n",
    "        \n",
    "        # Update rule\n",
    "        sig = sigma(np.dot(tx, w))\n",
    "        sig = sig - y\n",
    "        grad = np.dot(np.transpose(tx), sig)\n",
    "        w = w - gamma * grad \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_ , initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the solution using the regularized logistic regression using gradient descent method.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        yx = np.dot(y, np.transpose(tx))\n",
    "        yxw = np.dot(yx, w)\n",
    "        log = np.log(1 + np.exp(np.dot(np.transpose(tx), w)))\n",
    "        \n",
    "        # Add the 'penalty' term\n",
    "        loss = (log - yxw).sum() - (lambda_/2)* np.square((np.linalg.norm(w)))\n",
    "        \n",
    "        # Update rule\n",
    "        sig = sigma(np.dot(tx, w))\n",
    "        sig = sig - y\n",
    "        grad = np.dot(np.transpose(tx), sig) + 2 * lambda_*w\n",
    "        w = w - gamma * grad\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Retrieve input training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ty, tx, ids_train = load_csv_data(training_data, sub_sample = False)\n",
    "\n",
    "fy, fx, ids_test = load_csv_data(test_data, sub_sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "x_train = standardize(tx)\n",
    "fx_train = standardize(fx)\n",
    "\n",
    "# split data\n",
    "x_train, y_train, x_test, y_test = split_data(x_train, ty, 0.80, seed=1)\n",
    "\n",
    "# polynomial fit\n",
    "# degree = 2\n",
    "# x_train = build_poly(x_train, degree)\n",
    "# x_test = build_poly(x_test, degree)\n",
    "# tx_train = build_poly(tx_train, degree)\n",
    "\n",
    "# add intercept\n",
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "fx_train = np.hstack((np.ones((fx_train.shape[0], 1)), fx_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Least Squares Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.466%\n"
     ]
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "ls_accuracy = compare_prediction(w_ls, x_test, y_test)\n",
    "print('Accuracy: ' + str(ls_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares using Gradient Descent (using MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.182%\n"
     ]
    }
   ],
   "source": [
    "max_iters = 70\n",
    "step_size = 1e-5\n",
    "w_0 = np.ones(x_train.shape[1])\n",
    "\n",
    "w_lsgd, loss_lsgd = least_squares_GD(y_train, x_train, w_0, max_iters, step_size)\n",
    "lsGD_accuracy = compare_prediction(w_lsgd, x_test, y_test)\n",
    "print('Accuracy: ' + str(lsGD_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares using Stochastic Gradient Descent (batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.617999999999995%\n"
     ]
    }
   ],
   "source": [
    "max_iters = 50\n",
    "step_size = 1e-5\n",
    "w_0 = np.zeros(x_train.shape[1])\n",
    "\n",
    "w_lstoch, loss_lstoch = least_squares_SGD(y_train, x_train, w_0, max_iters, step_size) \n",
    "lsStoch_accuracy = compare_prediction(w_lstoch, x_test, y_test)\n",
    "print('Accuracy: ' + str(lsStoch_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.41199999999999%\n"
     ]
    }
   ],
   "source": [
    "degree = 12\n",
    "lambda_ = 0.278834626595\n",
    "w_rr, loss_rr = ridge_regression(y_train, build_poly(x_train, degree), lambda_)\n",
    "rr_accuracy = compare_prediction(w_rr, build_poly(x_test, degree), y_test)\n",
    "print('Accuracy: ' + str(rr_accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w_rr, build_poly(fx_train, degree))\n",
    "create_csv_submission(ids_test, y_pred, \"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
