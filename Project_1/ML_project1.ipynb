{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementations for implementations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Linear regression using gradient descent.\n",
    "    '''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad, loss = compute_gradient(y, tx ,w)\n",
    "        w = w - (gamma * grad)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Linear regression using stochastic gradient descent.\n",
    "    '''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            w = w - (gamma * grad)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    '''\n",
    "    Least squares regression using normal equations.\n",
    "    '''\n",
    "    gram = np.dot(np.transpose(tx),tx)\n",
    "    gram = np.linalg.inv(gram)\n",
    "    \n",
    "    w = np.dot(gram,np.transpose(tx))\n",
    "    w = np.dot(w, y)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''\n",
    "    Ridge regression using normal equations.\n",
    "    '''\n",
    "    N = tx.shape[1]\n",
    "    a = np.dot(np.transpose(tx), tx) + (lambda_ * np.identity(N))\n",
    "    b = np.dot(np.transpose(tx), y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Logistic regression using gradient descent.\n",
    "    '''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        yx = np.dot(y, tx)\n",
    "        yxw = np.dot(yx, w)\n",
    "        log = np.log(1 + np.exp(np.dot(tx, w)))\n",
    "        loss = (log - yxw).sum()\n",
    "        \n",
    "        # Update rule\n",
    "        sig = sigma(np.dot(tx, w))\n",
    "        sig = sig - y\n",
    "        grad = np.dot(np.transpose(tx), sig)\n",
    "        w = w - (gamma * grad)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_ , initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Regularized logistic regression using gradient descent.\n",
    "    '''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        yx = np.dot(y, tx)\n",
    "        yxw = np.dot(yx, w)\n",
    "        log = np.log(1 + np.exp(np.dot(tx, w)))\n",
    "        \n",
    "        # Add the 'penalty' term\n",
    "        loss = (log - yxw).sum() - (lambda_/2)* np.square((np.linalg.norm(w)))\n",
    "        \n",
    "        # Update rule\n",
    "        sig = sigma(np.dot(tx, w))\n",
    "        sig = sig - y\n",
    "        grad = np.dot(np.transpose(tx), sig) + (2 * lambda_*w)\n",
    "        w = w - (gamma * grad)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def error(y, tx, w):\n",
    "    '''\n",
    "    Calculates the error in the current prediction.\n",
    "    '''\n",
    "    return y - np.dot(tx, w)\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    '''\n",
    "    Calculates the loss using MSE.\n",
    "    '''\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = 1/(2*N)\n",
    "    loss = (np.dot(np.transpose(e), e)) * factor\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''\n",
    "    Computes the gradient of the MSE loss function.\n",
    "    '''\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = -1/N\n",
    "    grad = (np.dot(np.transpose(tx), e)) * factor\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''\n",
    "    Computes the stochastic gradient from a few examples of n and their corresponding y_n labels.\n",
    "    '''\n",
    "    N = y.shape[0]\n",
    "    e = error(y, tx, w)\n",
    "    factor = -1/N\n",
    "    grad = (np.dot(np.transpose(tx), e)) * factor\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "def sigma(x):\n",
    "    '''\n",
    "    Calculates sigma using the given formula.\n",
    "    '''\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    '''\n",
    "    Generates a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables - the output desired values 'y' and the input data 'tx'.\n",
    "    Outputs an iterator which gives mini-batches of batch_size matching elements from y and tx.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use:\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        do something\n",
    "    '''\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_prediction(w_train, x, y):\n",
    "    '''\n",
    "    Calculates the accuracy by comparing the predictions with given test data.\n",
    "    '''\n",
    "    pred = predict_labels(w_train, x)\n",
    "    N = len(pred)\n",
    "    count = 0.0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == y[i]:\n",
    "            count += 1\n",
    "    return count/N\n",
    "\n",
    "\n",
    "def split_data(tx, ty, ratio, seed=1):\n",
    "    '''\n",
    "    Split the training data by ratio.\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    split_idxs = [i for i in range(len(tx))]\n",
    "    \n",
    "    # Shuffle the indicies randomly\n",
    "    np.random.shuffle(split_idxs)\n",
    "    tx_shuffled = tx[split_idxs]\n",
    "    ty_shuffled = ty[split_idxs]\n",
    "    \n",
    "    # Split by ratio\n",
    "    split_pos = int(len(tx) * ratio)\n",
    "    x_train = tx_shuffled[:split_pos]\n",
    "    x_test = tx_shuffled[split_pos:]\n",
    "    y_train = ty_shuffled[:split_pos]\n",
    "    y_test = ty_shuffled[split_pos:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    '''\n",
    "    Builds a polynomial of the given degree and appends it to the given matrix.\n",
    "    '''\n",
    "    x_ret = x\n",
    "    for i in range(2, degree+1):\n",
    "        x_ret = np.c_[x_ret, np.power(x, i)]\n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def standardize(x):\n",
    "    '''\n",
    "    Standardizes the matrix by subtracting mean of each column and then dividing by standard deviation.\n",
    "    '''\n",
    "    res = x.copy()\n",
    "    for i in range(0, res.shape[1]):\n",
    "        if i == 22:\n",
    "            continue\n",
    "            \n",
    "        # Calculate mean and standard deviation without including NaN values\n",
    "        mean = np.nanmean(res[:,i])\n",
    "        std = np.nanstd(res[:,i])\n",
    "        \n",
    "        # Change mean and standard deviation if column has all NaN values\n",
    "        if np.isnan(mean):\n",
    "            mean = 0\n",
    "        if np.isnan(std) or std == 0:\n",
    "            std = 1\n",
    "        \n",
    "        # Replaces NaN values with mean and divides by standard deviation\n",
    "        for j in range(len(res[:,i])):\n",
    "            if np.isnan(res[j][i]):\n",
    "                res[j][i] = mean\n",
    "            else:\n",
    "                res[j][i] -= mean\n",
    "            res[j][i] /= std\n",
    "    return res\n",
    "\n",
    "\n",
    "def replace_999_with_nan(x):\n",
    "    '''\n",
    "    Replaces -999 (undefined values) with NaN.\n",
    "    '''\n",
    "    res = x.copy()\n",
    "    res[res == -999.0] = np.nan\n",
    "    return res\n",
    "\n",
    "\n",
    "def one_hot_encoding(x):\n",
    "    '''\n",
    "    Converts categorical data for PRI_jet_num (column at index 22) to use one hot encoding.\n",
    "    '''\n",
    "    res = x.copy()\n",
    "    col = res[:,22]\n",
    "    b = np.zeros((res.shape[0], 4))\n",
    "    for i in range(len(b)):\n",
    "        a = int(col[i])\n",
    "        b[i][a] = 1\n",
    "    res = np.delete(res, 22, 1)\n",
    "    res = np.hstack((res, b))\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_buckets(x):\n",
    "    '''\n",
    "    Splits the dataset into 8 buckets.\n",
    "    Based on 4 values (0, 1, 2, 3) of PRI_jet_num and 2 values (defined or -999) of DER_mass_MMC.\n",
    "    '''\n",
    "    result = []\n",
    "    for i in range(0, 4):\n",
    "        # Get all rows where PRI_jet_num equals i\n",
    "        x_jet = x[x[:,22] == i]\n",
    "        \n",
    "        # Get all rows where DER_mass_MMC defined and undefined\n",
    "        xi_defined = x_jet[x_jet[:,0] != -999.0]\n",
    "        xi_undefined = x_jet[x_jet[:,0] == -999.0]\n",
    "        \n",
    "        result.append(xi_defined)\n",
    "        result.append(xi_undefined)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_id_buckets(x):\n",
    "    '''\n",
    "    Splits the set of ids into 8 buckets as above.\n",
    "    Used for sorting the predictions based on event id.\n",
    "    '''\n",
    "    result = []\n",
    "    for i in range(0, 4):\n",
    "        x_jet = x[x[:,1] == i]\n",
    "        xi_defined = x_jet[x_jet[:,-1] != -999.0]\n",
    "        xi_undefined = x_jet[x_jet[:,-1] == -999.0]\n",
    "        result.append(xi_defined)\n",
    "        result.append(xi_undefined)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants for importing training and testing data\n",
    "TRAINING_DATA = '/Users/Gaurav/Desktop/train.csv'\n",
    "TEST_DATA = '/Users/Gaurav/Desktop/test.csv'\n",
    "\n",
    "# Load csv training and testing data\n",
    "ty, tx, ids_train = load_csv_data(TRAINING_DATA, sub_sample = False)\n",
    "fy, fx, ids_test = load_csv_data(TEST_DATA, sub_sample = False)\n",
    "\n",
    "# Backup of the imported training and testing data\n",
    "orig_tx = tx.copy()\n",
    "orig_ty = ty.copy()\n",
    "orig_fx = fx.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition into buckets, clean data, standardize, build polynomial, add intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice\n",
      "/anaconda/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1423: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Constants used to clean data\n",
    "SPLIT_PERCENT = 0.80\n",
    "DEGREE = 13\n",
    "LAMBDA_ = 0.9\n",
    "\n",
    "# Split data into 80-20 and use 80 for training and 20 to check model accuracy\n",
    "x_train, y_train, x_test, y_test = split_data(orig_tx.copy(), orig_ty.copy(), SPLIT_PERCENT, seed=1)\n",
    "\n",
    "# Split final test data to make predictions on into buckets\n",
    "fx_train = orig_fx.copy()\n",
    "fx_buckets = get_buckets(fx_train)\n",
    "\n",
    "# Append y values as column to later divide y into buckets corresponding with x values\n",
    "x_train = np.column_stack((x_train, y_train))\n",
    "x_test = np.column_stack((x_test, y_test))\n",
    "\n",
    "# Split training x into buckets\n",
    "buckets = get_buckets(x_train)\n",
    "\n",
    "# Split testing y into buckets corresponding to x values\n",
    "y_buckets = []\n",
    "for i in range(len(buckets)):\n",
    "    y_buckets.append(buckets[i][:,-1])\n",
    "    buckets[i] = np.delete(buckets[i], -1, 1)\n",
    "\n",
    "# Split testing x into buckets\n",
    "test_buckets = get_buckets(x_test)\n",
    "\n",
    "# Split testing y into buckets corresponding to x values\n",
    "test_y_buckets = []\n",
    "for i in range(len(test_buckets)):\n",
    "    test_y_buckets.append(test_buckets[i][:,-1])\n",
    "    test_buckets[i] = np.delete(test_buckets[i], -1, 1)\n",
    "\n",
    "# Replace -999 with NaN and standardize columns for each bucket\n",
    "for b in range(len(buckets)):\n",
    "    buckets[b] = replace_999_with_nan(buckets[b])\n",
    "    buckets[b] = standardize(buckets[b])\n",
    "#     buckets[b] = one_hot_encoding(buckets[b])\n",
    "    test_buckets[b] = replace_999_with_nan(test_buckets[b])\n",
    "    test_buckets[b] = standardize(test_buckets[b])\n",
    "#     test_buckets[b] = one_hot_encoding(test_buckets[b])\n",
    "    fx_buckets[b] = replace_999_with_nan(fx_buckets[b])\n",
    "    fx_buckets[b] = standardize(fx_buckets[b])\n",
    "#     fx_buckets[b] = one_hot_encoding(fx_buckets[b])\n",
    "    \n",
    "# Build polynomial of given degree for each bucket\n",
    "for b in range(len(buckets)):\n",
    "    buckets[b] = build_poly(buckets[b], DEGREE)\n",
    "    test_buckets[b] = build_poly(test_buckets[b], DEGREE)\n",
    "    fx_buckets[b] = build_poly(fx_buckets[b], DEGREE)\n",
    "\n",
    "# Add column of ones for intercept for each bucket\n",
    "for b in range(len(buckets)):\n",
    "    buckets[b] = np.column_stack((np.ones((buckets[b].shape[0], 1)), buckets[b]))\n",
    "    test_buckets[b] = np.column_stack((np.ones((test_buckets[b].shape[0], 1)), test_buckets[b]))\n",
    "    fx_buckets[b] = np.column_stack((np.ones((fx_buckets[b].shape[0], 1)), fx_buckets[b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression on each bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy = 0.82676 Degree = 13 Lambda = 0.9\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights for each bucket separately\n",
    "weights = []\n",
    "for i in range(len(buckets)):\n",
    "    w_rr, loss_rr = ridge_regression(y_buckets[i], buckets[i], LAMBDA_)\n",
    "    weights.append(w_rr)\n",
    "\n",
    "# Compare predictions for each bucket using its corresponding weights found earlier\n",
    "correct_predictions = 0\n",
    "len_data = 0\n",
    "for i in range(len(buckets)):\n",
    "    rr_accuracy = compare_prediction(weights[i], test_buckets[i], test_y_buckets[i])\n",
    "    correct_predictions += (rr_accuracy * len(test_buckets[i]))\n",
    "    len_data += len(test_buckets[i])\n",
    "\n",
    "total_accuracy = correct_predictions / len_data\n",
    "\n",
    "print(\"Total Accuracy = \" + str(total_accuracy) + \" Degree = \" + str(DEGREE) + \" Lambda = \" + str(LAMBDA_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions to Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output.csv with shape = (568238,)\n"
     ]
    }
   ],
   "source": [
    "# Create new array with Id, PRI_jet_num, and DER_mass_MMC for reordering predictions\n",
    "ids_array = ids_test\n",
    "pri_jet_num_col = orig_fx[:,22]\n",
    "der_mass_mmc_col = orig_fx[:,0]\n",
    "ids_array = np.column_stack((ids_array, pri_jet_num_col))\n",
    "ids_array = np.column_stack((ids_array, der_mass_mmc_col))\n",
    "\n",
    "# Divide Id into 8 buckets similar to input data\n",
    "id_buckets = get_id_buckets(ids_array)\n",
    "\n",
    "# Make predictions for each bucket using weights calculated by training on each bucket\n",
    "final_y = predict_labels(weights[0], fx_buckets[0])\n",
    "final_y = np.column_stack((final_y, id_buckets[0]))\n",
    "for i in range(1, len(weights)):\n",
    "    y_pred = predict_labels(weights[i], fx_buckets[i])\n",
    "    y_pred = np.column_stack((y_pred, id_buckets[i]))\n",
    "    final_y = np.concatenate((final_y, y_pred))\n",
    "    \n",
    "# Sort predictions based on Id\n",
    "final_y = final_y[final_y[:,1].argsort()]\n",
    "\n",
    "# Select only prediction values\n",
    "final_y = final_y[:,0]\n",
    "\n",
    "# Create output file containing predictions\n",
    "create_csv_submission(ids_test, final_y, \"output.csv\")\n",
    "print(\"Created output.csv with shape = \" + str(final_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
